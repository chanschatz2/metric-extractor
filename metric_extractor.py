import os
import re
import json
import time
import argparse
from dataclasses import dataclass
from typing import List, Dict, Any

import numpy as np
import pdfplumber
import faiss

from sentence_transformers import SentenceTransformer
from jsonschema import validate
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList

from true_metrics import TRUE_METRICS   # for accuracy eval

# 
# Configuration
# 

MODEL_ID = "Qwen/Qwen3-0.6B"
CHUNK_SIZE = 1000         # characters per text / prose chunk
TOP_K = 6                 # max total chunks to send to LLM (keep this small)
PER_QUERY_K = 2           # chunks per query variant
#TEMPERATURE = float(os.getenv("TEMP", "0.0"))
MAX_NEW_TOKENS = 256#1024
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# The metrics (by id) for this LLM to extract, by grouping
METRICS_GROUP = {
    "Custom": [1, 2, 3, 4, 18],
    "All": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],
    "Energy&Utilities" : [1,2,3,4,5,6,7,18,19,23,24,25,27,28,29,30,31,32],
    "Manufacturing" : [1,2,3,4,5,6,7,9,11,13,15,20,21,23,24,25,27,28,29,30,33],
    "Automotive" : [1,2,3,4,5,6,7,8,10,12,14,20,21,23,24,25,26,27,28,29,30,34,35],
    "Services" : [1,2,3,4,5,6,7,8,10,12,16,17,20,21,22,23,24,25,27,28,29,30]
}

# SELECT PRESET GROUP
try:
    EXTRACT_METRICS = METRICS_GROUP["Custom"]
except KeyError:
    print("ERROR: CHOSEN METRIC GROUP NOT FOUND.")

# The list of metrics (taken from question map)
TARGETS = [
    (1,  "Waste generated - Hazardous [metric ton]"),
    (2,  "Waste generated - Non Hazardous [metric ton]"),
    (3,  "Diverted Waste from Disposal (reused, recycled, recovered) [metric ton]"),

    (4,  "Water withdrawal [m3]"),
    (5,  "Total FRESH water discharge (<= 1,000mg/L TDS) [m3]"),
    (6,  "Total OTHER water discharge (>= 1,000mg/L TDS) [m3]"),
    (7,  "Water Recycled or Reused [m3]"),

    (8,  "Non-renewable materials used [metric ton]"),
    (9,  "Non-renewable packaging material used [metric ton]"),
    (10, "Renewable material used [metric ton]"),
    (11, "Renewable packaging material used [metric ton]"),
    (12, "Recycled input material used [metric ton]"),
    (13, "Recycled packaging material used [metric ton]"),
    (14, "Reusable, compostable or recyclable material [%]"),
    (15, "Reusable, compostable or recyclable material [%]"),
    (16, "Paper consumption [metric ton]"),
    (17, "Single-use plastic consumption [metric ton]"),

    (18, "Total energy generated by the company [MWh]"),
    (19, "Total non fossil fuel energy generated by the company [MWh]"),
    (20, "Total energy consumed [MWh]"),
    (21, "Renewable energy consumed [MWh]"),
    (22, "Certified buildings and facilities i.e. LEED [%]"),

    (23, "Total direct GHG emissions [tCO2e]"),
    (24, "Total energy indirect GHG emissions [tCO2e]"),
    (25, "Total use of products [metric tons CO2 equilvalent tCO2e]"),
    (26, "Average specific CO2 emissions [gCO2/km]"),
    (27, "Emissions neutralized by Carbon Offset Projects [tCO2e]"),
    (28, "Emissions of ozone-depleting substances (ODS) [metric tons of CFC-11]"),
    (29, "Nitrogen oxides [NOX], sulfur oxides [SOX] and other significant air emissions [tons NOx + Sox]"),

    (30, "Spillage and fines [$]"),
    (31, "Volume of flared hydrocarbon [tCO2e]"),
    (32, "Volume of vented hydrocarbon [tCO2e]"),

    (33, "Packaging material to be reclaimed/recovered [%]"),
    (34, "Material to be reclaimed [%]"),
    (35, "Average lifespan of product or warranty provided [years]")
]


## To assist and automate conversions
#UNIT_MAP: Dict[int, Tuple[str, Dict[str, float]]] = {
#    20: ("MWh",   {"MWh": 1.0, "GWh": 1000.0, "kWh": 0.001, "MWh/yr": 1.0, "MWh/year": 1.0}),
#    23: ("tCO2e", {"tCO2e": 1.0, "ktCO2e": 1000.0, "MtCO2e": 1_000_000.0, "tonnes CO2e": 1.0}),
#    4:  ("m3",    {"m3": 1.0, "m^3": 1.0, "Mm3": 1_000_000.0, "cubic meters": 1.0}),
#}

# Scale conversion
SCALE_MAP = {
    "thousand": 1_000,
    "million": 1_000_000,
    "billion": 1_000_000_000,
    #"k": 1_000,
    #"kt": 1_000,    # kiloton
    #"m": 1_000_000, # million
    #"mt": 1_000_000,    # megaton
    #"gwh": 1_000,   # GWh -> MWh
    #"twh": 1_000_000    # TWh -> MWh
}

# For metric variant creation
UNIT_TEXT_MAP = {
    "[metric ton]": ["metric ton", "tonnes"],
    "[m3]": ["m3", "cubic meters"],
    "[mwh]": ["MWh", "megawatt hours"],
    "[%]": ["%", "percent"],
    "[tco2e]": ["tCO2e", "metric tons CO2e"],
    "[metric tons co2 equilvalent tco2e]": ["tCO2e""tonnes CO2e"],
    "[gco2/km]": ["gCO2/km", "grams of CO2 per kilometer"],
    "[metric tons of cfc-11]": ["metric tons of CFC-11"],
    "[tons nox + sox]": ["tons of NOx and SOx"],
    "[$]": ["$", "USD", "dollars"],
    "[years]": ["years", "year(s)"],
}

# 
# PDF Chunking (text + table)
# 

# Splits text into sentences and keeps sentence chunks under chunk_size
def split_keep_sentences(text: str, chunk_size: int) -> List[str]:
    text = text or ""
    if not text.strip():
        return []
    out, buf = [], []

    # remove newlines and split sentences on periods
    parts = [p.strip() for p in text.replace("\n", " ").split(". ") if p.strip()]

    # re-combines sentences
    # keeps under chunk_size to avoid truncation
    for sent in parts:
        current_len = sum(len(s) for s in buf) + 2*max(0, len(buf)-1)
        if current_len + len(sent) + 2 > chunk_size and buf:
            out.append(". ".join(buf))
            buf = []
        buf.append(sent)
    if buf:
        out.append(". ".join(buf))
    return out

# Associates chunk text with an index
@dataclass
class Chunk:
    text: str
    idx: int

# For each page in PDF:
    # Extracts tables row‑by‑row and preserves row / col order in a chunk
    # Extracts prose (text) and splits into chunks w/ above split_keep_sentences
def extract_chunks(pdf_path: str, chunk_size: int = CHUNK_SIZE) -> List[Chunk]:
    chunks: List[Chunk] = []
    idx = 0
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            # Create table chunks
            tables = page.extract_tables() or []
            for t in tables:
                for row in t:
                    row_txt = " | ".join([(c or "").strip() for c in row])
                    if row_txt.strip():
                        chunks.append(Chunk(text=f"[TABLE p{page_num}] {row_txt}", idx=idx))
                        idx += 1

            # Create text chunks
            txt = page.extract_text() or ""
            for block in split_keep_sentences(txt, chunk_size):
                if block.strip():
                    chunks.append(Chunk(text=f"[TEXT p{page_num}] {block}", idx=idx))
                    idx += 1
    return chunks

# 
# Chunk Embedding w/ FAISS index
# 

class Retriever:
    # Encodes chunk texts using embedding model
    # Builds FAISS vector DB to hold encoded text for retrieval
    def __init__(self, chunks: List[Chunk], model_name: str = EMBED_MODEL_NAME):
        self.chunks = chunks
        self.model = SentenceTransformer(model_name)
        self.dim = self.model.get_sentence_embedding_dimension()
        texts = [c.text for c in chunks]
        embs = self.model.encode(texts, normalize_embeddings=True)  # cosine via IP
        self.index = faiss.IndexFlatIP(self.dim)
        self.index.add(embs.astype(np.float32))

    # Encode query for search & retrieve top-k nearest chunks (from vector DB)
    def search(self, q: str, k: int) -> List[Chunk]:
        q_emb = self.model.encode([q], normalize_embeddings=True).astype(np.float32)
        scores, idxs = self.index.search(q_emb, k)
        idxs = idxs[0].tolist()
        return [self.chunks[i] for i in idxs if 0 <= i < len(self.chunks)]
    
# Assist year finding with variants
def year_variants(y: int) -> list[str]:
    return [str(y), f"FY{y}", f"CY{y}"]

# Build many query varients for the given metric
    # raises chances of meaningful FAISS chunk recalls even if the PDF phrases differently
    # creates around 3 - 6 variants
def build_queries(metric_name: str, target_year: int) -> list[str]:
    variants = [metric_name]

    # find the first [ ]] pair if exists
    if "[" in metric_name and "]" in metric_name:
        start = metric_name.index("[")
        end = metric_name.index("]", start)
        bracket = metric_name[start:end+1]
        key = bracket.lower() # normalize to lowercase for search in unit map

        # unit replacements
        for rep in UNIT_TEXT_MAP.get(key, []):
            v = metric_name[:start] + rep + metric_name[end+1:]
            variants.append(v.strip())

        # no-bracket variant
        variants.append((metric_name[:start] + metric_name[end+1:]).strip())

    # Add year variants + bias for last year 
    yrs = year_variants(target_year) + year_variants(target_year - 1)

    # puts the year variants before and after each of the above base variants
    yr_variants = []
    for b in variants:
        for y in yrs:
            yr_variants.append(f"{b} {y}")

    # de-dupe & preserve order
    seen = set()
    out = []
    for s in yr_variants:
        if s not in seen:
            out.append(s); seen.add(s)

#    print("Query Variants: " + str(out))
    return out

YEAR_RE = re.compile(r"\b(?:CY|FY)?(20\d{2})\b") # pattern to match any year

# Rank chunks based on year proximity to target year
    # helps push more relevant chunks toward the beginning
def proximity_score(text: str, target_year: int) -> int:
    years = [int(y) for y in YEAR_RE.findall(text)] # find all instances of years
    score = 0
    if str(target_year) in text or f"FY{target_year}" in text or f"CY{target_year}" in text:
        score += 5

    # prefer nearest <= target_year
    priors = [y for y in years if y <= target_year]
    if priors:
        gap = target_year - max(priors)
        score += max(0, 3 - gap)  # 3,2,1 bonus for 0/1/2-year gaps

    # gently penalize future years or many different years in one chunk
    score -= sum(1 for y in years if y > target_year)
    score -= max(0, len(set(years)) - 2)
    return score


# Run query variants through FAISS vector DB and retrive their most relevant PDF chunks
# De-duplicate them, then only keep the top total_cap chunks (TOP_K)
    # This keeps the most relevant chunks between all of the query variants
def retrieve_context(ret: Retriever, metric_name: str, target_year: int, per_query_k: int = PER_QUERY_K, total_cap: int = TOP_K) -> List[str]:
    contexts: List[str] = []
    for q in build_queries(metric_name, target_year):
        contexts.extend(h.text for h in ret.search(q, k=per_query_k))
    seen, stitched = set(), []
    for c in contexts:
        if c not in seen:
            stitched.append(c); seen.add(c)

    # sort by year proximity
    stitched.sort(key=lambda t: proximity_score(t, target_year), reverse=True)
    return stitched[:total_cap]

# 
# Qweb3-0.6B Querying
# 

# StoppingCriteria subclass to halt text generation after JSON is created
    # limits incomplete generation -> leads to null JSON values on schema enforcement
class StopOnFirstJSON(StoppingCriteria):
    def __init__(self, tokenizer, prompt_len: int):
        self.tok = tokenizer
        self.prompt_len = prompt_len    # number of tokens in the prompt

    # Abstract class method override
    # return True -> stop generation
    def __call__(self, input_ids, scores, **kwargs) -> bool:
        # only look at text after the prompt
        gen_ids = input_ids[0][self.prompt_len:]
        if gen_ids.numel() == 0:
            return False
        text = self.tok.decode(gen_ids, skip_special_tokens=True)

        # search for first paired bracket JSON object
        depth = 0
        saw_open = False
        for ch in text:
            if ch == '{':
                saw_open = True
                depth += 1
            elif ch == '}':
                if saw_open:
                    depth -= 1
                    if depth == 0:   # first object closed -> stop now
                        return True
        return False

# Hardware accelerator
    # mps -> apple silicon
    # cuda -> NVDA gpu
    # cpu when others not available
def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps")
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

class LocalQwen:
    def __init__(self, model_id: str = MODEL_ID):
        self.device = get_device()
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if self.device.type in ("cuda", "mps") else torch.float32,
            device_map="auto" if self.device.type in ("cuda", "mps") else None,
            trust_remote_code=True,
        )
        if self.device.type == "cpu":
            self.model.to(self.device)

    # Generate text with system and user prompt, up to max_new_tokens
        # prepends '{' to ensure JSON output
    def chat(self, system: str, user: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ]

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        prompt += "{" # force model to start with json response

        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True).to(self.device)

        json_stopper = StopOnFirstJSON(self.tokenizer, inputs["input_ids"].shape[1])

        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id,
                stopping_criteria=StoppingCriteriaList([json_stopper])
            )

        text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        # strip the prompt part if it echoes
        return text[len(prompt):].strip()

# 
# JSON Schema & parsing helpers
# 

def metric_schema() -> Dict[str, Any]:
    return {
        "type": "object",
        "properties": {
            "metric_id": {"type": "integer"},
            "metric_name": {"type": "string"},
            "value": {"type": "number"},
            "unit": {"type": "string"},
            "reported_year": {"type": "integer"},
        },
        "required": ["metric_id", "metric_name", "value", "unit", "reported_year"]
    }

#JSON_EXTRACT_RE = re.compile(r'\{.*\}', re.DOTALL)

#def extract_first_json(text: str) -> Dict[str, Any]:
#    m = JSON_EXTRACT_RE.search(text)
#    if not m:
#        return {}
#    frag = m.group(0)
#    try:
#        return json.loads(frag)
#    except Exception:
#        # Try to repair common trailing commas or quotes issues
#        try:
#            frag2 = frag.replace('\n', ' ').replace(', }', ' }').replace(', ]', ' ]')
#            return json.loads(frag2)
#        except Exception:
#            return {}

def extract_last_json(text: str):
    """Return the last valid top-level JSON object in text, or {}."""
    # Strip think tags that sometimes appear
    cleaned = text.replace("</think>", "").replace("<think>", "")
    stack = []
    start = None
    last_good = None

    for i, ch in enumerate(cleaned):
        if ch == '{':
            if not stack:  # top-level open
                start = i
            stack.append('{')
        elif ch == '}':
            if stack:
                stack.pop()
                if not stack and start is not None:
                    candidate = cleaned[start:i+1]
                    try:
                        obj = json.loads(candidate)
                        last_good = obj  # keep the last successfully parsed object
                    except json.JSONDecodeError:
                        pass
                    start = None
    return last_good or {}

def enforce_schema(obj: Dict[str, Any]) -> Dict[str, Any]:
    try:
        validate(instance=obj, schema=metric_schema())
        return obj
    except Exception as e:
        print(f"VALIDATION ERROR: {e}")
        return {"metric_id": "", "value": 0, "unit": "", "reported_year": 0}
    
# Unit normalization
# 

# Convert units using UNIT_MAP
#def normalize(metric_id: int, value: float, unit: str) -> Tuple[float, str]:
#    if unit is None:
#        return value, unit
#    target = UNIT_MAP.get(metric_id)
#    if not target:
#        return value, unit
#    canon_unit, conv = target
#    u = unit.strip()
#    if u in conv:
#        return value * conv[u], canon_unit
#    for alt, factor in conv.items():
#        if u.lower() == alt.lower():
#            return value * factor, canon_unit
#    return value, unit
    
def normalize(value, unit):
    # Convert scale words and metric prefixes into base units
    if value is None or int(value) == 0:
        return None, unit

    val = float(value)
    unit_lower = unit.lower().strip() if unit else ""

    for key, multiplier in SCALE_MAP.items():
        if key in unit_lower:
            val *= multiplier
            # Remove the scale word from unit label
            unit_lower = unit_lower.replace(key, "").strip()
            break

    return val, unit_lower

# 
# Extractor pipeline using local Qwen 
# 

#     return f"""
#Extract exactly one numeric metric as JSON for the requested metric, using ONLY the explicit information in the context.
#Only return a value if it is explicitly labeled for the year {target_year}. If multiple years are present, choose the cell/column for the year {target_year}.
#"If the requested year {target_year} is not explicitly present, return an empty JSON object."

def build_user_prompt(metric_id: int, metric_name: str, context: str, target_year: int) -> str:
    return f"""Requested metric: {metric_name}
Target year: {target_year}

TASK
Return exactly one JSON object for the requested metric using ONLY values explicitly shown in the context.

SELECTION RULE:
- Look only for the requested metric in the provided context.
- If it is explicitly reported for multiple years, pick the value from the latest year among the target year {target_year} or the previous year {target_year - 1}.
- Do not choose a year just because it appears later in the text—use only the year(s) where the metric’s value is actually given.
- Record that year in reported_year.
- If no numeric value for this metric is explicitly shown for year {target_year} or {target_year-1}, return {{}}.


CONSTRAINTS
- Prefer table cells over narrative sentences.
- Ignore targets/future years (> {target_year}), baselines, and YTD/quarterly figures.
- Output numbers as plain digits (no commas). Do not guess or compute.

OUTPUT
Return ONLY one JSON object on a single line with these keys:
{{
  "metric_id": {metric_id},
  "metric_name": "{metric_name}",
  "value": <number>,
  "unit": "<unit string>",
  "reported_year": <integer>
}}

Example (no value available)
Context:
Metric | 2021 | 2022
Water withdrawal [m3] | — | —
Target year: 2024
Expected: {{}}

Context:
{context}

Return ONLY the JSON object. No explanations, no extra text.
"""

def build_verifier_prompt(prediction: str, chunk: str, metric_name: str, metric_value, target_year: int) -> str:
    return f"""
Requested metric:
- Name: {metric_name}
- Value: {str(metric_value)}

TASK
Return exactly one JSON object with your answer to verifying whether an extracted metric value is supported by the given context.

VERIFICATION RULES:
- Assume the metric is unsupported unless the context explicitly supports this metric.
- Do not look for specific word matches, instead use the *meaning* of the requested metric.
- Confirm ONLY if the context contains information that successfully answers the requested metric. Synonyms are OKAY.
- Differences in units are expected and acceptable.

Context chunk:
{chunk}

OUTPUT:
Return only one JSON object:
{{
  "verified": true/false,
  "reason": "<brief reason>"
}}

Return ONLY the JSON object. No explanations, no extra text.
"""

"""
- Deny if the context could support similar, but different *meaning* metrics.


Examples:
- If context says "Amount of hazardous waste manifested for disposal (tons): 11.6" and metric is "Waste generated - Hazardous [metric ton]" -> verified: true
- If context says "non-hazardous waste generated: 50 metric tons" but metric is "Waste generated - Hazardous [metric ton]" -> verified: false
- If no value appears for the requested metric -> verified: false
"""


# OTHER PROMPTS ARE IN scripts/test_prompts.txt

def extract_metric(model: LocalQwen, metric_id: int, metric_name: str, ctx_chunks: List[str], target_year: int) -> Dict[str, Any]:
    context = "\n---\n".join(ctx_chunks)
    system = (
        "You are a precise information extractor. "
        "You only output valid JSON when asked and avoid adding extra text."
    )
    user = build_user_prompt(metric_id, metric_name, context, target_year)
    raw = "{" + model.chat(system, user) # reattach opening brace
    #raw = model.chat(system, user)

    print("-- PASS 1: RAW --")
    print(raw)

    # obj = extract_first_json(raw)
    obj = extract_last_json(raw)
    obj = enforce_schema(obj)
    print("-- PASS 1: EXTRACTED METRIC --")
    print(obj)
    return obj

def chunk_contains_value(chunk: str, metric_value: str) -> bool:
    if metric_value is None:
        return False
    
    # remove commas and spaces
    chunk_norm = chunk.replace(",", "").replace("\u00A0", " ").replace("\u202F", " ").replace(" ", "")
    val_norm = metric_value.strip().replace(",", "").replace(" ", "")

    return val_norm in chunk_norm

def find_chunks_with_value(chunks: list[str], metric_value: str) -> list[str]:
    if not chunks:
        return []
    
    hits = []
    for ch in chunks:
        if chunk_contains_value(ch, metric_value):
            hits.append(ch)
    return hits

# Verify extracted metric with another query
def verify_metric(model: LocalQwen, prediction: dict, chunk_text: str, metric_name: str, target_year: int) -> tuple[bool, str]:
    # No value to verify
    if prediction.get("value") is None:
        return False, "value is none"

    system = "You are a strict verifier that only outputs a single compact JSON object. No explanations."
    metric_value = prediction.get("value")
    str_json = json.dumps(prediction, ensure_ascii=False)
    user = build_verifier_prompt(str_json, chunk_text, metric_name, metric_value, target_year)

    raw = "{" + model.chat(system=system, user=user)
    print("-- PASS 2: CONTEXT --")
    print(chunk_text)
    print("-- PASS 2 VERIFY RAW --")
    print(raw)

    j = extract_last_json(raw) or {}
    #print(f"j: {j}")
    verdict = j.get("verified", "false") # if key not present, false
    reason  = j.get("reason", "")

    return (verdict), (reason or "no_reason")


#
# Testing
#

def test_values(pdf_path: str, predictions: List[Dict[str, Any]]):
    file_name = os.path.basename(pdf_path) # get just the filename from pat
    #print("pred:")
    #print(predictions)
    
    true = []
    for true_json in TRUE_METRICS.get(file_name):
        if true_json["metric_id"] in EXTRACT_METRICS:
            true.append(true_json)


    #print("true:")
    #print(true)

    results = []
    correct = 0

    for i, p_json in enumerate(predictions): # should be in same order as true since same grouping
        #print("Prediction JSON:")
        #print(p_json)
        #curr_id = p_json["metric_id"]
        #print("curr_id:")
        #print(curr_id)

        #print(f"i: {i}")
        t_value = true[i]["value"]
        if t_value == "null" or isinstance(t_value, str):
            t_value = None

        print(f"True value: {t_value}")

        p_value = p_json["value"]
        if p_value == "null":
            p_value = None

        print(f"Predicted value: {p_value}")

        if t_value == p_value:
            if t_value == None:
                results.append("Correct - None")
            elif t_value != None:
                results.append("Correct - Match")
            else:
                results.append("Correct")
            correct = correct + 1
        else:
            if t_value is not None:
                results.append("Incorrect - True Non-None")
            else:
                results.append("Incorrect")
        
    print("-- TESTING RESULTS --")
    print(f"Results: {results}")
    print(f"Accuracy: {correct / len(predictions)}")


# 
# Main
# 

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("pdf", help="Path to sustainability report PDF")
    ap.add_argument("--year", type=int, required=True, help="Target reporting year")
    ap.add_argument("--test", action="store_true", help="Evaluate predictions against truths")
    args = ap.parse_args()

    print(f"Loading PDF and building chunks from: {args.pdf}")
    chunks = extract_chunks(args.pdf, chunk_size=CHUNK_SIZE)
    print(f"Total chunks: {len(chunks)}")

    target_year = args.year

    print(f"\nLoading embeddings: {EMBED_MODEL_NAME}")
    retriever = Retriever(chunks, model_name=EMBED_MODEL_NAME)
    print("FAISS index built.")

    print(f"\nLoading local model: {MODEL_ID}")
    qwen = LocalQwen(MODEL_ID)

    results = []
    extract_targets = [TARGETS[i - 1] for i in EXTRACT_METRICS]

    total_time = time.time()
    for metric_id, metric_name in extract_targets:
        print(f"\n=== Extracting: {metric_name} (ID {metric_id}) ===")
        ctx_chunks = retrieve_context(retriever, metric_name, target_year, per_query_k=PER_QUERY_K, total_cap=TOP_K)
        if not ctx_chunks:
            print("No context retrieved; skipping.")
            continue

#        print("-- CONTEXT CHUNKS --")
#        print(ctx_chunks)

        # Query LLM / Begin pipeline
        t0 = time.time()
        # extract prediction json
        out = extract_metric(qwen, metric_id, metric_name, ctx_chunks, target_year) or {}

        # find chunks that contain the metric
        best_chunks = find_chunks_with_value(ctx_chunks, str(out["value"]))
        print("-- BEST CHUNKS (verifying with these) --")
        print(best_chunks)

        # verify with second qweb call
        verdict, why = verify_metric(qwen, out, best_chunks, metric_name, target_year)
        print(f"Verdict: {verdict}")

        # If not found
        if not verdict:
            print(f"Rejecting prediction value: {out.get('value')} for {metric_name}")
            print(f"     - reason: {why}")
            out["value"] = None
            out["unit"] = None
            out["reported_year"] = None
        
        dt = time.time() - t0 # time for both queries

        value = out.get("value")
        unit = out.get("unit")
        value_norm, unit_norm = (None, unit)
        if value is not None and unit is not None:
            value_norm, unit_norm = normalize(value, unit)

        row = {
            "metric_id": metric_id,
            "metric_name": metric_name,
            "value": value_norm,
            "unit": unit,
#            "value_norm": value_norm,
#            "unit_norm": unit_norm,
            "reported_year": target_year,
            "latency_s": round(dt, 3),
            "chunks_used": ctx_chunks,
        }
        results.append(row)

        print(json.dumps({
            "metric": metric_name,
            "value": value,
            "unit": unit,
#            "value_norm": value_norm,
#            "unit_norm": unit_norm,
            "reported_year": target_year,
            "latency_s": row["latency_s"],
        }, ensure_ascii=False))

    dt = time.time()
    print(f"-- TOTAL LATENCY/TIME: {round(dt-total_time, 3)} --")

    if args.test: 
        test_values(args.pdf, predictions=results)

    out_path = os.path.splitext(os.path.basename(args.pdf))[0] + "_rag_results_local_qwen3.jsonl"
    with open(out_path, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
    print(f"\nSaved results to: {out_path}")

if __name__ == "__main__":
    main()

    
# python metric_extractor.py --year 2024 "reports/NextEra Energy Inc 2024 Sustainability Report (SustainabilityReports.com).pdf" 
# remember to use environment rag-test -> conda activate rag-test