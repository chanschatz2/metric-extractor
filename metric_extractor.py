import os
import re
import json
import time
import argparse
from dataclasses import dataclass
from typing import List, Dict, Any
import math

import numpy as np
import pdfplumber
import faiss

from sentence_transformers import SentenceTransformer
from jsonschema import validate
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList

from true_metrics import TRUE_METRICS   # for accuracy eval

# 
# Configuration
# 

MODEL_ID = "Qwen/Qwen3-0.6B"
CHUNK_SIZE = 1000         # characters per text / prose chunk
TOP_K = 6                 # max total chunks to send to LLM (keep this small)
PER_QUERY_K = 2           # chunks per query variant
#TEMPERATURE = float(os.getenv("TEMP", "0.0"))
MAX_NEW_TOKENS = 256#1024
#EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2" # 384 dim embedder
EMBED_MODEL_NAME = "Alibaba-NLP/gte-large-en-v1.5" # 1024 dim embedder

DEBUG = False  # global debug flag

# The metrics (by id) for this LLM to extract, by grouping
METRICS_GROUP = {
    "Custom": [4],
    "All": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],
    "Energy&Utilities" : [1,2,3,4,5,6,7,18,19,23,24,25,27,28,29,30,31,32],
    "Manufacturing" : [1,2,3,4,5,6,7,9,11,13,15,20,21,23,24,25,27,28,29,30,33],
    "Automotive" : [1,2,3,4,5,6,7,8,10,12,14,20,21,23,24,25,26,27,28,29,30,34,35],
    "Services" : [1,2,3,4,5,6,7,8,10,12,16,17,20,21,22,23,24,25,27,28,29,30]
}

# SELECT PRESET GROUP
try:
    #EXTRACT_METRICS = METRICS_GROUP["Custom"]
    EXTRACT_METRICS = METRICS_GROUP["Energy&Utilities"]
except KeyError:
    print("ERROR: CHOSEN METRIC GROUP NOT FOUND.")

# The list of metrics (taken from question map)
TARGETS = [
    (1,  "Waste generated - Hazardous [metric ton]"),
    (2,  "Waste generated - Non Hazardous [metric ton]"),
    (3,  "Diverted Waste from Disposal (reused, recycled, recovered) [metric ton]"),

    (4,  "Water withdrawal [m3]"),
    (5,  "Total FRESH water discharge (<= 1,000mg/L TDS) [m3]"),
    (6,  "Total OTHER water discharge (>= 1,000mg/L TDS) [m3]"),
    (7,  "Water Recycled or Reused [m3]"),

    (8,  "Non-renewable materials used [metric ton]"),
    (9,  "Non-renewable packaging material used [metric ton]"),
    (10, "Renewable material used [metric ton]"),
    (11, "Renewable packaging material used [metric ton]"),
    (12, "Recycled input material used [metric ton]"),
    (13, "Recycled packaging material used [metric ton]"),
    (14, "Reusable, compostable or recyclable material [%]"),
    (15, "Reusable, compostable or recyclable material [%]"),
    (16, "Paper consumption [metric ton]"),
    (17, "Single-use plastic consumption [metric ton]"),

    (18, "Total energy generated by the company [MWh]"),
    (19, "Total non fossil fuel energy generated by the company [MWh]"),
    (20, "Total energy consumed [MWh]"),
    (21, "Renewable energy consumed [MWh]"),
    (22, "Certified buildings and facilities i.e. LEED [%]"),

    (23, "Total direct GHG emissions [tCO2e]"),
    (24, "Total energy indirect GHG emissions [tCO2e]"),
    (25, "Total use of products [metric tons CO2 equilvalent tCO2e]"),
    (26, "Average specific CO2 emissions [gCO2/km]"),
    (27, "Emissions neutralized by Carbon Offset Projects [tCO2e]"),
    (28, "Emissions of ozone-depleting substances (ODS) [metric tons of CFC-11]"),
    (29, "Nitrogen oxides [NOX], sulfur oxides [SOX] and other significant air emissions [tons NOx + Sox]"),

    (30, "Spillage and fines [$]"),
    (31, "Volume of flared hydrocarbon [tCO2e]"),
    (32, "Volume of vented hydrocarbon [tCO2e]"),

    (33, "Packaging material to be reclaimed/recovered [%]"),
    (34, "Material to be reclaimed [%]"),
    (35, "Average lifespan of product or warranty provided [years]")
]

# Scale conversion
SCALE_MAP = {
    "thousand": 1_000,
    "million": 1_000_000,
    "billion": 1_000_000_000,
}

# For metric variant creation
UNIT_TEXT_MAP = {
    "[metric ton]": ["metric ton", "tonnes"],
    "[m3]": ["m3", "cubic meters"],
    "[mwh]": ["MWh", "megawatt hours"],
    "[%]": ["%", "percent"],
    "[tco2e]": ["tCO2e", "metric tons CO2e"],
    "[metric tons co2 equilvalent tco2e]": ["tCO2e""tonnes CO2e"],
    "[gco2/km]": ["gCO2/km", "grams of CO2 per kilometer"],
    "[metric tons of cfc-11]": ["metric tons of CFC-11"],
    "[tons nox + sox]": ["tons of NOx and SOx"],
    "[$]": ["$", "USD", "dollars"],
    "[years]": ["years", "year(s)"],
}

#
# DEBUG
#

def log_debug(*args, **kwargs) -> None:
    # Print only when DEBUG is True
    if DEBUG:
        print(*args, **kwargs)

# 
# PDF Chunking (text + table)
# 

# Splits text into sentences and keeps sentence chunks under chunk_size
def split_keep_sentences(text: str, chunk_size: int) -> List[str]:
    text = text or ""
    if not text.strip():
        return []
    out, buf = [], []

    # remove newlines and split sentences on periods
    parts = [p.strip() for p in text.replace("\n", " ").split(". ") if p.strip()]

    # re-combines sentences
    # keeps under chunk_size to avoid truncation
    for sent in parts:
        current_len = sum(len(s) for s in buf) + 2*max(0, len(buf)-1)
        if current_len + len(sent) + 2 > chunk_size and buf:
            out.append(". ".join(buf))
            buf = []
        buf.append(sent)
    if buf:
        out.append(". ".join(buf))
    return out

# Associates chunk text with an index
@dataclass
class Chunk:
    text: str
    idx: int

# For each page in PDF:
    # Extracts tables row‑by‑row and preserves row / col order in a chunk
    # Extracts prose (text) and splits into chunks w/ above split_keep_sentences
def extract_chunks(pdf_path: str, chunk_size: int = CHUNK_SIZE) -> List[Chunk]:
    chunks: List[Chunk] = []
    idx = 0
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            # Create table chunks
            tables = page.extract_tables() or []
            for t in tables:
                for row in t:
                    row_txt = " | ".join([(c or "").strip() for c in row])
                    if row_txt.strip():
                        chunks.append(Chunk(text=f"[TABLE p{page_num}] {row_txt}", idx=idx))
                        idx += 1

            # Create text chunks
            txt = page.extract_text() or ""
            for block in split_keep_sentences(txt, chunk_size):
                if block.strip():
                    chunks.append(Chunk(text=f"[TEXT p{page_num}] {block}", idx=idx))
                    idx += 1
    return chunks

# 
# Chunk Embedding w/ FAISS index
# 

class Retriever:
    # Encodes chunk texts using embedding model
    # Builds FAISS vector DB to hold encoded text for retrieval
    def __init__(self, chunks: List[Chunk], model_name: str = EMBED_MODEL_NAME):
        self.chunks = chunks
        self.model = SentenceTransformer(model_name, trust_remote_code=True)
        #self.model.max_seq_length = 512 # bit safer on runtime for long chunks
        self.dim = self.model.get_sentence_embedding_dimension()
        texts = [c.text for c in chunks]
        embs = self.model.encode(texts, normalize_embeddings=True)  # cosine via IP
        self.index = faiss.IndexFlatIP(self.dim)
        self.index.add(embs.astype(np.float32))

    # Encode query for search & retrieve top-k nearest chunks (from vector DB)
    def search(self, q: str, k: int) -> List[Chunk]:
        q_emb = self.model.encode([q], normalize_embeddings=True).astype(np.float32)
        scores, idxs = self.index.search(q_emb, k)
        idxs = idxs[0].tolist()
        return [self.chunks[i] for i in idxs if 0 <= i < len(self.chunks)]
    
# Assist year finding with variants
def year_variants(y: int) -> list[str]:
    return [str(y), f"FY{y}", f"CY{y}"]

# Build many query varients for the given metric
    # raises chances of meaningful FAISS chunk recalls even if the PDF phrases differently
    # creates around 3 - 6 variants
def build_queries(metric_name: str, target_year: int) -> list[str]:
    variants = [metric_name]

    # find the first [ ]] pair if exists
    if "[" in metric_name and "]" in metric_name:
        start = metric_name.index("[")
        end = metric_name.index("]", start)
        bracket = metric_name[start:end+1]
        key = bracket.lower() # normalize to lowercase for search in unit map

        # unit replacements
        for rep in UNIT_TEXT_MAP.get(key, []):
            v = metric_name[:start] + rep + metric_name[end+1:]
            variants.append(v.strip())

        # no-bracket variant
        variants.append((metric_name[:start] + metric_name[end+1:]).strip())

    # Add year variants + bias for last year 
    yrs = year_variants(target_year) + year_variants(target_year - 1)

    # puts the year variants before and after each of the above base variants
    yr_variants = []
    for b in variants:
        for y in yrs:
            yr_variants.append(f"{b} {y}")

    # de-dupe & preserve order
    seen = set()
    out = []
    for s in yr_variants:
        if s not in seen:
            out.append(s); seen.add(s)

    log_debug(f"== QUERY VARIANTS FOR METRIC {metric_name} ==")
    log_debug(out)

    return out

YEAR_RE = re.compile(r"\b(?:CY|FY)?(20\d{2})\b") # pattern to match any year

# Rank chunks based on year proximity to target year
    # helps push more relevant chunks toward the beginning
def proximity_score(text: str, target_year: int) -> int:
    years = [int(y) for y in YEAR_RE.findall(text)] # find all instances of years
    score = 0
    if str(target_year) in text or f"FY{target_year}" in text or f"CY{target_year}" in text:
        score += 5

    # prefer nearest <= target_year
    priors = [y for y in years if y <= target_year]
    if priors:
        gap = target_year - max(priors)
        score += max(0, 3 - gap)  # 3,2,1 bonus for 0/1/2-year gaps

    # gently penalize future years or many different years in one chunk
    score -= sum(1 for y in years if y > target_year)
    score -= max(0, len(set(years)) - 2)
    return score

# Rank chunks based on their likelihood to contain the metic:
    # overlap with metric name, presence of numbers/units, table-like patterns
def semantic_score(metric_name: str, text: str) -> float:
    tl = text.lower()
    name_tokens = [t for t in re.split(r'[\W_]+', metric_name.lower()) if t]
    score = 0.0

    # token overlap
    for tok in name_tokens:
        if tok and tok in tl:
            score += 1.0

    # numeric / unit cues
    if re.search(r'\d', tl):
        score += 0.5
    if any(u in tl for u in ['ton', 'mwh', 'ghg', 'co2', '%', 'percent', 'm3', 'tco2e', 'usd', '$']):
        score += 0.5

    # table-ish (number followed by text)
    if re.search(r'\d+\s+[A-Za-z]', tl):
        score += 0.3

    return score


# Run query variants through FAISS vector DB and retrive their most relevant PDF chunks
# De-duplicate them, then only keep the top total_cap chunks (TOP_K)
    # This keeps the most relevant chunks between all of the query variants
def retrieve_context(ret: Retriever, metric_name: str, target_year: int, per_query_k: int = PER_QUERY_K, total_cap: int = TOP_K) -> List[str]:
    contexts: List[str] = []
    for q in build_queries(metric_name, target_year):
        contexts.extend(h.text for h in ret.search(q, k=per_query_k))
    seen, stitched = set(), []
    for c in contexts:
        if c not in seen:
            stitched.append(c); seen.add(c)

    # sort by (year proximity, semantic similarity to the metric)
    stitched.sort(
        key=lambda t: (proximity_score(t, target_year), semantic_score(metric_name, t)),
        reverse=True
    )
    #stitched.sort(key=lambda t: proximity_score(t, target_year), reverse=True)

    return stitched[:total_cap]

# 
# Qweb3-0.6B Querying
# 

# StoppingCriteria subclass to halt text generation after JSON is created
    # limits incomplete generation -> leads to null JSON values on schema enforcement
class StopOnFirstJSON(StoppingCriteria):
    def __init__(self, tokenizer, prompt_len: int):
        self.tok = tokenizer
        self.prompt_len = prompt_len    # number of tokens in the prompt

    # Abstract class method override
    # return True -> stop generation
    def __call__(self, input_ids, scores, **kwargs) -> bool:
        # only look at text after the prompt
        gen_ids = input_ids[0][self.prompt_len:]
        if gen_ids.numel() == 0:
            return False
        text = self.tok.decode(gen_ids, skip_special_tokens=True)

        # search for first paired bracket JSON object
        depth = 0
        saw_open = False
        for ch in text:
            if ch == '{':
                saw_open = True
                depth += 1
            elif ch == '}':
                if saw_open:
                    depth -= 1
                    if depth == 0: # first object closed -> stop now
                        return True
        return False

# Hardware accelerator
    # mps -> apple silicon
    # cuda -> NVDA gpu
    # cpu when others not available
def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps")
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

class LocalQwen:
    def __init__(self, model_id: str = MODEL_ID):
        self.device = get_device()
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if self.device.type in ("cuda", "mps") else torch.float32,
            device_map="auto" if self.device.type in ("cuda", "mps") else None,
            trust_remote_code=True,
        )
        if self.device.type == "cpu":
            self.model.to(self.device)

    # Generate text with system and user prompt, up to max_new_tokens
        # prepends '{' to ensure JSON output
    def chat(self, system: str, user: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ]

        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        prompt += "{" # force model to start with json response

        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True).to(self.device)

        json_stopper = StopOnFirstJSON(self.tokenizer, inputs["input_ids"].shape[1])

        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id,
                stopping_criteria=StoppingCriteriaList([json_stopper])
            )

        text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        # strip the prompt part if it echoes
        return text[len(prompt):].strip()

# 
# JSON Schema & parsing helpers
# 

# json schema enforced by various parts of the program
def metric_schema() -> Dict[str, Any]:
    return {
        "type": "object",
        "properties": {
            "metric_id": {"type": "integer"},
            "metric_name": {"type": "string"},
            "value": {"type": ["number", "null"]},
            "unit": {"type": ["string", "null"]},
            "reported_year": {"type": ["integer", "null"]},
        },
        "required": ["value", "unit", "reported_year"]
    }

def extract_last_json(text: str):
    """Return the last valid top-level JSON object in text, or {}."""
    # Strip think tags that sometimes appear
    cleaned = text.replace("</think>", "").replace("<think>", "")
    stack = []
    start = None
    last_good = None

    for i, ch in enumerate(cleaned):
        if ch == '{':
            if not stack:  # top-level open
                start = i
            stack.append('{')
        elif ch == '}':
            if stack:
                stack.pop()
                if not stack and start is not None:
                    candidate = cleaned[start:i+1]
                    try:
                        obj = json.loads(candidate)
                        last_good = obj  # keep the last successfully parsed object
                    except json.JSONDecodeError:
                        pass
                    start = None
    return last_good or {}

def enforce_schema(obj: Dict[str, Any]) -> Dict[str, Any]:
    try:
        # If this passes, we trust the object as-is
        validate(instance=obj, schema=metric_schema())
        return obj
    except Exception as e:
        log_debug("VALIDATION ERROR:", e)

        # use .get to protect from weird input
        metric_id = obj.get("metric_id")
        metric_name = obj.get("metric_name")
        value = obj.get("value")
        unit = obj.get("unit")
        reported_year = obj.get("reported_year")

        if DEBUG:
            reason = f"schema_validation_failed: {e}"
        else:
            reason = "schema_validation_failed"

        out = {
            "metric_id": metric_id,
            "metric_name": metric_name,
            "value": None if value is None else value,
            "unit": None if unit is None else unit,
            "reported_year": None if reported_year is None else reported_year,
            "status": "parse_error",
            "reason": reason,
        }
        return out
    
#
# Unit normalization
# 


def normalize(value, unit):
    # Convert scale words and metric prefixes into base units
    if value is None or int(value) == 0:
        return None, unit

    val = float(value)
    unit_lower = unit.lower().strip() if unit else ""

    for key, multiplier in SCALE_MAP.items():
        if key in unit_lower:
            val *= multiplier
            # Remove the scale word from unit label
            unit_lower = unit_lower.replace(key, "").strip()
            break

    return val, unit_lower

# 
# Extractor pipeline using local Qwen 
# 


def build_user_prompt(metric_id: int, metric_name: str, context: str, target_year: int) -> str:
    safe_context = context.replace("```", "'") # make sure string doesnt break
    return f"""
You are a sustainability metric extraction model.

TASK
- Find the value of the metric "{metric_name}" (id={metric_id}) for reporting year {target_year} in the CONTEXT below.
- Use ONLY information explicitly written in CONTEXT. Do NOT infer, estimate, or compute values.

RULES
- If the metric and the value for year {target_year} are clearly shown, copy the numeric value exactly (keep decimal point, ignore thousand separators like "," or spaces).
- If multiple years are shown, use the row/column for year {target_year} only.
- If the metric appears but the year {target_year} is missing, return null for value and reported_year.
- If you cannot confidently find this metric for year {target_year}, return null for value and reported_year.
- Never invent values. When in doubt, choose null.

OUTPUT FORMAT (strict JSON)
Return EXACTLY one JSON object with keys:

{{
  "metric_id": {metric_id},
  "metric_name": "{metric_name}",
  "value": <number or null>,
  "unit": <string or null>,
  "reported_year": <integer or null>
}}

CONTEXT
{safe_context}
"""


def build_verifier_prompt(chunk: str, metric_name: str, metric_value, reported_year: int, target_year: int) -> str:
    safe_chunk = chunk.replace("```", "'") # ensure string doesnt break
    return f"""
You are verifying an ESG metric using a single text chunk.

CANDIDATE
- metric_name: {metric_name}
- value: {metric_value}

TASK
Decide if this candidate is DIRECTLY SUPPORTED by the CHUNK below.
Carefully scan the CHUNK for all numeric values.

"Directly supported" means ALL of the following are true:
1. The numeric value appears in the chunk (ignoring thousand separators like "," or spaces).
2. The chunk is describing the SAME quantity as metric_name (not employees, revenue, capacity, etc.).

If any of these conditions is missing, unclear or ambiguous, you MUST treat the metric as NOT supported.

OUTPUT FORMAT (strict JSON)
Return EXACTLY one JSON object:

{{
  "verified": true or false,
  "reason": "<short explanation>"
}}

CHUNK
{safe_chunk}
"""


# OTHER PROMPTS ARE IN scripts/test_prompts.txt

def extract_metric(model: LocalQwen, metric_id: int, metric_name: str, ctx_chunks: List[str], target_year: int) -> Dict[str, Any]:
    context = "\n---\n".join(ctx_chunks)
    system = (
        "You are a precise information extractor. "
        "You only output valid JSON when asked and avoid adding extra text."
    )
    user = build_user_prompt(metric_id, metric_name, context, target_year)
    raw = "{" + model.chat(system, user) # reattach opening brace

    log_debug("-- PASS 1: RAW --")
    log_debug(raw)

    # obj = extract_first_json(raw)
    obj = extract_last_json(raw)
    obj = enforce_schema(obj)

    # avoid not passing checks later because of metric_id metric_name
    obj.setdefault("metric_id", metric_id)
    obj.setdefault("metric_name", metric_name)

    log_debug("-- PASS 1: EXTRACTED METRIC --")
    log_debug(obj)
    return obj

def chunk_contains_value(chunk: str, v_str: str) -> bool:
    # tolerances
    rel_tol = 1e-6
    abs_tol = 1e-6

    # v_str -> metric value to find

    if v_str is None:
        return False

    # Parse target value
    try:
        target = float(str(v_str))
    except ValueError:
        return False

    # Extract all numeric-looking substrings from chunk
    pattern = r"-?\d[\d,]*(?:\.\d+)?"
    nums = re.findall(pattern, chunk)
    if not nums:
        return False

    for n in nums:
        try:
            val = float(n.replace(",", ""))
        except ValueError:
            continue

        # direct numeric match
        if math.isclose(val, target, rel_tol=rel_tol, abs_tol=abs_tol):
            return True

        # allow simple powers of 10 scaling (thousand / million)
        if target != 0:
            ratio = val / target
            # e.g., 7,900,212 vs 7,900,212,000 or 95.6 vs 95,600,000
            for factor in (1e3, 1e6, 1e-3, 1e-6):
                if math.isclose(ratio, factor, rel_tol=1e-2):
                    return True

    return False

def find_chunks_with_value(chunks: list[str], metric_value: str) -> list[str]:
    if not chunks:
        return []
    
    hits = []
    for ch in chunks:
        if chunk_contains_value(ch, metric_value):
            hits.append(ch)
    return hits

# Verify extracted metric with another query
def verify_metric(model: LocalQwen, prediction: dict, chunk_text: str, metric_name: str, target_year: int) -> tuple[bool, str]:
    # No value to verify
    if prediction.get("value") is None:
        return False, "value is none"

    system = "You are a strict verifier that only outputs a single compact JSON object. No explanations."
    metric_value = prediction.get("value")
    reported_year = prediction.get("reported_year")

    user = build_verifier_prompt(
        chunk=chunk_text,
        metric_name=metric_name,
        metric_value=metric_value,
        reported_year=reported_year,
        target_year=target_year,
    )

    raw = model.chat(system=system, user=user)
    log_debug("-- PASS 2: CONTEXT --")
    log_debug(chunk_text)
    log_debug("-- PASS 2 VERIFY RAW --")
    log_debug(raw)

    # Extract the last JSON object from the raw string
    j = extract_last_json(raw) or {}
    raw_verdict = j.get("verified", False)

    # Ensure is proper bool
    if isinstance(raw_verdict, str):
        verdict = raw_verdict.strip().lower() == "true"
    else:
        verdict = bool(raw_verdict)

    reason = j.get("reason") or "no_reason"

    return verdict, reason


#
# Testing
#

def test_values(pdf_path: str, predictions: List[Dict[str, Any]]) -> None:
    file_name = os.path.basename(pdf_path)
    true_metrics = TRUE_METRICS.get(file_name, [])

    if not true_metrics:
        print(f"[TEST] No ground-truth metrics found for {file_name}")
        return

    # Only evaluate metrics you asked the pipeline to extract
    true_by_id: Dict[int, Dict[str, Any]] = {
        m["metric_id"]: m
        for m in true_metrics
        if m["metric_id"] in EXTRACT_METRICS
    }

    pred_by_id: Dict[int, Dict[str, Any]] = {
        p["metric_id"]: p for p in predictions
    }

    n = len(true_by_id)
    if n == 0:
        print(f"[TEST] No overlapping metric_ids between TRUE_METRICS and EXTRACT_METRICS for {file_name}")
        return

    retrieval_hits = 0 # TRUE value appears in any chunks_used
    llm_exact = 0 # predicted value == TRUE value (normalized, year-aware)
    llm_exact_given_retrieval = 0

    task_correct = 0 # correct extractions + correct denials
    value_cases = 0 # how many metrics have a non-None true value
    value_correct = 0

    print("\n=== PER-METRIC EVAL (retrieval vs extraction) ===")
    for mid, t in sorted(true_by_id.items()):
        gt_val = t.get("value")
        gt_year = t.get("year")
        gt_unit = t.get("unit")

        p = pred_by_id.get(mid)
        if not p:
            print(f"[id={mid}] no prediction produced by pipeline")
            continue

        pred_val = p.get("value")
        pred_year = p.get("reported_year")
        pred_unit = p.get("unit")
        status = p.get("status")
        chunks_used = p.get("chunks_used") or []

        # Retrieval recall: does ANY chunk_used contain the TRUE value?
        retrieval_hit = False
        if gt_val is not None:
            v_str = str(gt_val)
            for ch in chunks_used:
                if chunk_contains_value(v_str=v_str, chunk=ch):
                    retrieval_hit = True
                    break

        if retrieval_hit:
            retrieval_hits += 1

        # LLM extraction accuracy (numeric + year)
        year_match = (gt_year is None) or (gt_year == pred_year)

        numeric_match = False
        if gt_val is not None and pred_val is not None:
            try:
                numeric_match = math.isclose(float(pred_val), float(gt_val), rel_tol=1e-6, abs_tol=1e-6)
            except ValueError:
                numeric_match = False

        exact_match = numeric_match and year_match

        if gt_val is not None:
            value_cases += 1
            if exact_match:
                value_correct += 1

        if exact_match:
            llm_exact += 1
            if retrieval_hit:
                llm_exact_given_retrieval += 1

        # "accepting" value -> status == "accepted" AND pred_val is not None.
        accepted = (status == "accepted" and pred_val is not None)

        if gt_val is None:
            this_correct = not accepted
        else:
            this_correct = exact_match

        if this_correct:
            task_correct += 1

        print(
            f"[id={mid}] "
            f"true=({gt_val}, {gt_unit}, {gt_year}) "
            f"pred=({pred_val}, {pred_unit}, {pred_year}) "
            f"retrieval_hit={retrieval_hit} "
            f"llm_exact={exact_match} "
            f"accepted={accepted} "
            f"task_correct={this_correct} "
            f"status={status} "
            f"reason={p.get('reason')}"
        )

    print("\n=== SUMMARY ===")
    print(f"Total metrics evaluated: {n}")

    # Retrieval recall: only over metrics that actually have a TRUE numeric value
    if value_cases:
        print(
            "Retrieval recall "
            "(TRUE value appears in any chunks_used, for metrics with TRUE numeric value): "
            f"{retrieval_hits}/{value_cases} = {retrieval_hits / value_cases:.2f}"
        )
    else:
        print(
            "Retrieval recall "
            "(TRUE value appears in any chunks_used): N/A (no metrics with TRUE numeric value)"
        )

    if value_cases:
        print(
            "LLM exact-match accuracy "
            "(normalized value + year, for metrics with TRUE numeric value): "
            f"{llm_exact}/{value_cases} = {llm_exact / value_cases:.2f}"
        )
    else:
        print(
            "LLM exact-match accuracy "
            "(normalized value + year): N/A (no metrics with TRUE numeric value)"
        )

    # Conditional accuracy given retrieval
    if retrieval_hits:
        print(
            "LLM accuracy GIVEN retrieval_hit: "
            f"{llm_exact_given_retrieval}/{retrieval_hits} = "
            f"{llm_exact_given_retrieval / retrieval_hits:.2f}"
        )
    else:
        print(
            "LLM accuracy GIVEN retrieval_hit: N/A "
            "(no cases where TRUE value appeared in chunks_used)"
        )

    if value_cases:
        print(
            f"\nValue accuracy on metrics with a TRUE numeric value: "
            f"{value_correct}/{value_cases} = {value_correct / value_cases:.2f}"
        )
    else:
        print(
            "\nValue accuracy on metrics with a TRUE numeric value: "
            "N/A (no metrics with TRUE numeric value)"
        )

    print(
        "Overall TASK-LEVEL accuracy "
        "(correct value when TRUE exists, correct denial when TRUE is None): "
        f"{task_correct}/{n} = {task_correct / n:.2f}"
    )


# 
# Main
# 

def main():
    global DEBUG

    ap = argparse.ArgumentParser()
    ap.add_argument("pdf", help="Path to sustainability report PDF")
    ap.add_argument("--year", type=int, required=True, help="Target reporting year")
    ap.add_argument("--test", action="store_true", help="Evaluate predictions against truths")
    ap.add_argument("--debug", action="store_true", help="Print detailed debug logs (raw LLM outputs, schema errors, etc.)")
    args = ap.parse_args()

    DEBUG = args.debug

    if not DEBUG:
        try:
            from transformers import logging as hf_logging
            hf_logging.set_verbosity_error()
        except Exception:
            pass

    # global pipeline timer
    pipeline_start = time.time()

    print(f"Loading PDF and building chunks from: {args.pdf}")
    t_pdf0 = time.time()
    chunks = extract_chunks(args.pdf, chunk_size=CHUNK_SIZE)
    pdf_build_dt = time.time() - t_pdf0
    print(f"Total chunks: {len(chunks)}")
    print(f"[TIMING] PDF to chunks: {pdf_build_dt:.3f} s")

    target_year = args.year

    print(f"\nLoading embeddings: {EMBED_MODEL_NAME}")
    t_emb0 = time.time()
    retriever = Retriever(chunks, model_name=EMBED_MODEL_NAME)
    emb_build_dt = time.time() - t_emb0
    print("FAISS index built.")
    print(f"[TIMING] Embeddings + FAISS index: {emb_build_dt:.3f} s")

    print(f"\nLoading local model: {MODEL_ID}")
    t_llm0 = time.time()
    qwen = LocalQwen(MODEL_ID)
    llm_load_dt = time.time() - t_llm0
    print(f"[TIMING] Local model load: {llm_load_dt:.3f} s")

    results = []
    extract_targets = [TARGETS[i - 1] for i in EXTRACT_METRICS]

    # Lists for per-metric timing
    metric_latencies = [] # full pipeline per metric (retrieval + passes)
    retrieval_latencies = [] # retrieval-only per metric

    for metric_id, metric_name in extract_targets:
        print(f"\n=== Extracting: {metric_name} (ID {metric_id}) ===")
        metric_start = time.time()
        t_ret0 = time.time()

        ctx_chunks = retrieve_context(retriever, metric_name, target_year, per_query_k=PER_QUERY_K, total_cap=TOP_K)
        
        retrieval_dt = time.time() - t_ret0
        retrieval_latencies.append(retrieval_dt)
        
        if not ctx_chunks:
            print("No context retrieved; skipping.")
            metric_dt = time.time() - metric_start
            metric_latencies.append(metric_dt)
            continue

        # Query LLM / Begin pipeline

        # extract prediction json
        out = extract_metric(qwen, metric_id, metric_name, ctx_chunks, target_year) or {}

        # If Pass 1 failed to produce a valid JSON metric, skip verification
        if out.get("status") == "parse_error" and out.get("value") is None:
            print(f"[SKIP VERIFY] parse_error for metric {metric_id} - {metric_name}")

            metric_dt = time.time() - metric_start
            metric_latencies.append(metric_dt)

            # still want to record this attempt
            row = {
                "metric_id": metric_id,
                "metric_name": metric_name,
                "value": None,
                "unit": None,
                "reported_year": target_year,
                "latency_s": round(metric_dt, 3),
                "chunks_used": ctx_chunks,
                "status": out.get("status"),
                "reason": out.get("reason"),
                "confidence": None,
                "citation": None,
            }
            results.append(row)
            continue  # move on to the next metric


        # find chunks that contain the metric
        best_chunks = find_chunks_with_value(ctx_chunks, str(out["value"]))
        log_debug("-- BEST CHUNKS (verifying with these) --")
        log_debug(best_chunks)

        # verify with second qweb call
        hits = len(best_chunks)
        out["confidence"] = round(min(1.0, 0.4 + 0.2 * hits), 2)

        verdict = False
        why = "not_supported_by_context"

        if best_chunks:
            for chk in best_chunks[:3]:  # try up to 3 strongest literal matches
                v, w = verify_metric(qwen, out, chk, metric_name, target_year)
                if v:
                    verdict, why = True, "supported_and_verified"
                    break
        else:
            # no literal match in context -> immediate rejection
            verdict, why = False, "not_supported_by_context"

        print(f"Verdict: {verdict}")

        if not verdict:
            print(f"Verification failed for value: {out.get('value')} for {metric_name}")
            print(f"     - reason: {why}")
            # DO NOT wipe the value; just mark it as unverified
            out["status"] = "unverified"
            out["reason"] = why
        else:
            out["status"] = "accepted"
            out["reason"] = why

        metric_dt = time.time() - metric_start
        metric_latencies.append(metric_dt)
        print(f"[TIMING] Full pipeline for metric {metric_id}: {metric_dt:.3f} s")

        value = out.get("value")
        unit = out.get("unit")
        value_norm, unit_norm = (None, unit)
        if value is not None and unit is not None:
            value_norm, unit_norm = normalize(value, unit)

        row = {
            "metric_id": metric_id,
            "metric_name": metric_name,
            "value": value_norm,
            "unit": unit,
            "reported_year": target_year,
            "latency_s": round(metric_dt, 3),
            "chunks_used": ctx_chunks,
            "status": out.get("status"),
            "reason": out.get("reason"),
            "confidence": out.get("confidence")
        }
        results.append(row)

        log_debug(json.dumps({
            "metric": metric_name,
            "value": value,
            "unit": unit,
            "reported_year": target_year,
            "latency_s": row["latency_s"],
            "chunks_used": ctx_chunks,
            "status": row["status"],
            "reason": row["reason"],
            "confidence": row["confidence"]
        }, ensure_ascii=False))

    # --- SUMMARY TIMING ---
    pipeline_dt = time.time() - pipeline_start
    print(f"\n-- TOTAL PIPELINE TIME: {pipeline_dt:.3f} s --")

    if metric_latencies:
        avg_metric = sum(metric_latencies) / len(metric_latencies)
        print(f"[TIMING] Avg full-metric latency: {avg_metric:.3f} s over {len(metric_latencies)} metrics")
    if retrieval_latencies:
        avg_retr = sum(retrieval_latencies) / len(retrieval_latencies)
        print(f"[TIMING] Avg retrieval latency: {avg_retr:.3f} s over {len(retrieval_latencies)} metrics")

    if args.test: 
        test_values(args.pdf, predictions=results)

    out_path = os.path.splitext(os.path.basename(args.pdf))[0] + "_rag_results_local_qwen3.jsonl"
    with open(out_path, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
    print(f"\nSaved results to: {out_path}")

if __name__ == "__main__":
    main()


# python metric_extractor.py --year 2024 "reports/NextEra Energy Inc 2024 Sustainability Report (SustainabilityReports.com).pdf"  --test
# python metric_extractor.py --year 2023 "reports/nestle_report_2023.pdf" --test
# remember to use environment rag-test -> conda activate rag-test